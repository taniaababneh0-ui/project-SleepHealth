## OWEN | TANIA

1-Preprocessing:
#Owen(60%):Data ingestion & memory-savvy joins,eda,Feature Eng,Dimensionality collinearity,Imbalanced data handling,Time-aware splitting

#Tania(40%): Cleaning & Imputation,Encoding Categorical Variables, Feature scaling

2-Task A:
#Owen (90%) :  Logistic Regression,K-Nearest Neighbors classifier,Random Forest classifier,Support Vector Machine , Decision Tree classifier,XGBOOST,LightGBM)
#Tania(10%) :pdf Classification Models, pdf Evaluation and Explainability          

3- Task B:
#Owen (90%) :Logistic Regression,K-Nearest Neighbors classifier,Random Forest classifier,Support Vector Machine , Decision Tree classifier,XGBOOST,LightGBM)

#Tania(10%) :pdf Regression Models ,pdf Evaluation and Explainability


                                             Preprocessing:

# ReduceMemory:
هذه المرحلة لتقليل استهلاك الذاكرة , بما أن حجم البيانات كبير جدا قد يؤدي تحميل هذه البيانات كما هي الى استهلاك الكثير من الذاكرة رام
مما قد يتسبب في تعطيل البرنامج (crush)
لذلك يتم استخدام هذه الدالة التي تقوم بتقليص حجم البيانات عن طريق تحويل انواع الاعمدة الى الانواع التي تستخدم ذاكرة اقل 

# MergeData:
قمت بدمج عدة داتا  فريم في داتا فريم واحدة complete_df 
ب استخدام primarykey في كل ملف 
تم دمج بيانات product &order_product اولا 
ثم دمجت باقي البيانات بناءا على المفاتيح المشتركة بين الجداول 
تركت مجموعة البيانات order_train 
منفصلة لاستخدامها في تدريب النموذج

3* 
بعد ذلك قمت ب استكشاف البيانات عن طريق فحص طبيعتها بشكل دقيق كما قمت بفتحها من 
Excel للحصول على نظرة اوسع عنها

# EDA
قمت بتحليل استكشافي للبيانات لمواصلة قراءة وفهم البيانات وتحديد العوامل التي سيعتمد عليها النموذج في عملية التنبؤ

5*
من خلال تحليل البيانات اكتشفت ان العمود days_since_prior_order
يحتوي على العديد من القيم المفقودة هذا العمود يمثل الفترة الزمنية او الفجوة بين الطلب الحالي والطلب السابق
 يمكن تفسير هذه القيم المفقودة على انها تشير الى ان الطلب الحالي هو او طلب للعميل او ان العميل لم يكرر الطلب بعد طلبه السابق 

#Tania:
#  Cleaning & Imputation
1-Define and justify imputation techniques for missing fields (median, mode, sentinel, 
model-based).

- Median Imputation
في هذه المرحلة قمت بتعويض القيمه المفقود في العمود
days_since_prior_orderلتحل محل القيم المفقودة بقيمة الوسيط لهذا العمود
كما قمت بعرض مخطط توزيع بياني للعمود بعد تطبيق التعويض ب استخدام الوسيط 

- Most Frequent (Mode) Imputation
في هذه المرحلة قمت بتعويض القيمه المفقود في العمود
days_since_prior_order
ب استخدام اكثر قيمة تكرار
كما قمت بتعرض مخطط توزيع بياني بعد تطبيق التعويض ب استخدام الاكثر قيمة تكرار 


- Sentinel Imputation

في هذه المرحلة قمت بتعويض القيمه المفقود في العمود
days_since_prior_order
ب استخدام القيمة 0 لتعويض القيمة المفقودة كما قمت ايضا بعرض مخطط توزيع بياني 

- Model-Based Imputation (KNN Imputer)

في هذه المرحلة قمت بتعويض القيمه المفقود في العمود
days_since_prior_order
ب استخدام خوارزمية الجيران الاقرب لتعويض القيمة المفقودة كما قمت ايضا بعرض مخطط التوزيع البياني 


قمت بتطبيق جميع تقنيات التعويض بالقيم المفقودة على نسخة من البيانات بهدف عرض تأثير كل منها كما طلب الدكتور
قررت في النهاية تعويض القيم المفقودة في العمود days_since_prior_order
بـ 0 بدلاً من الوسيط او القيمة اكثر تكرار او خوارزمية الجيران الاقرب لانه رح يعطو قيم تشير الى اعادة طلب غير منطقية
لذلك قمنا بتعويض القيم ب 0 اكثر منطقية حيث يشير الى عدم وجود طلب سابق

# Outlier Detection & Treatment (Z-Score Method)

اولا قمت ب استخدام طريقة الارباع لاكتشاف الاعمدة التي تحتوي على الاوتلير في الداتا سيت 
بعد ذلك قمت بحساب Z-score
للأعمدة التي تحتوي  على القيم المتطرفة بعد حساب المتوسط والانحراف المعياري لها 
تم تحديد القيم المتطرفة بناءا على الزي سكور الاكبر من 3 او اقل من -3 بعدها عرضت مخطط 
BoxPlot &histoلتوزيع البيانات قبل ازالة القيم المتطرفة مما يساعد في تصور تأثيرها على البيانات 
واذا منلاحظ بالرسمة النسبه في الاوتلير ب add_to_cart_orderكانت تقريبا 60 وصار في فجوة ل 80 وهاد بدل على انه يوجد قيم متططرفة 

الان اتم تصفية البيانات بحيث تشمل فقط القيم التي تكون بين -3 و 3 في الزي سكور مما يعني ازالة القيم التي تحتوي على الزي سكور اكبر من 3 واقل من -3 
وبالتالي تيثى القيم التي لا تحتوي على الاوتلير 
بعد ذلك قمت بعرض مخطط boxplot & histo
للبيانات المتبقية بعد ازالة الاوتلير 
صار في عندنا فرق بعد ما شلنا الاوتلير القيم ب add_to_cart_orderكانت كثير عالية بعدد ما زلنا الاوتلير
انخفضتت وحتى بالهستو قبل ازالة الاوتلير كان الفريكونسي 7000 وبعد ازالة الاوتلير اصبح 50000


# One-Hot Encoding
#One-Hot Encoding (for low-cardinality categories)
استخدنا الون هوت انكودينج لفئات منخقضة التعددية كما طلب الدكتور في الملف 
قمناا ب اختيار الاعمدة aisle,departmentلانهما يحتويان على عدد قليل من الفئات المقيدة 
تم استخدام دالة get_dummies لتطبيق هذه التقنية 
مع تحديد المعامل drop_first=True لحل مشكلة التكرار بين الاعمدة في حال وجود فئات مشابهة 
هذا التكرار رح يؤدي الى تعداد الخطي عند استخدام الانحدار الخطي مما يصعب على النموذج تحديد التأثير الفعلي لكل متغير بدقة وؤدي الى 
مشكلة الافراط في التكيف overfitting او underfitting نقص في التعلم 
بالاضافة الى ذلك استخدمنا sparse matrix لتقليل استهلاك الذاكرة عند التعامل مع البيانات بعد تطبيق الون هوت انكودينج
بدل ماا يخزن القيم صفرية و 1 في الميموري يتم تخزين فقط القيم الغير صفرية فقط مع مواقعها

# 5. Feature Scaling

Feature Scaling("StandardScaler")
 استخدمت الستادرد سكيلر لتطبيق الفيجر سكيلنق على البيانات
 هذه  لعملية تقوم بتحويل البيانات بحيث يصبح المتوسط 0 والانحراف المعياري 1 
 مما يساعد في تحسين اذا النموذج وخاصة عند استخدام خوارزميات حساسة 

Feature Scaling("MinMaxScaler")
هذه العملية تقوم باغيير الاعمدة بحيث اصبحت القيم تتراوح بين 0 و 1 مما يسهل معالجتها من قبل النماذج التي تتطلب مقياسا موحدا 

قمت بتطبيق كل من StandardScaler و MinMaxScaler
 على نسخ منفصلة من البيانات لعرض تأثير كل منهما على توزيع البيانات
لكن رح نستخدم  StandardScaler لانه هوه اقل حساسية للاوتلير  

# OWEN:

# 6- Feature Engineering

نفذنا جميع عمليات Feature ENG
اعتمادا على بيانات prior فقط ليش؟
عشان نتجنب الداتا ليكج لانه لو استخدمنا بيانات من الطلب الحالي او المستقبل هيك المودل بصير يشوف معلومات مش لازم يعرفها وقت التدريب

1-'User Level Features'
*. Total Orders per User: هنا استخدمنا الماكس للحصول على اعلى رقم طلب اي اخر طلب قام به المستخدم وبالتالي نعتبر ان هذا الرقم يمثل اجمالي عدد الطلبات للمستخدم 
*- Average Basket Size:  بيمثل متوسط عدد المنتجات اللي المستخدم بحطها بكل طلب وهذا الاشي بيعكسلنا نمط التسوق عنده اذا كان بشتري طلبات صغيرة او كبيرة 
*- Reorder Ratio:
 هنا حسبنا نسبة اعادة الطلب لكل مستخدم ب استخدام mean
 *-mean days  mean days between ordersهون حسبنا المتوسط الايام بين الطلبات 
 last order recency هون حسبنا كم مر وقت بالايام بين اخر طلب وطلب اللي قبله
-'Product Level Features'

*overall reorder rate: هنا حسبنا نسبة اعادة الطلب لكل منتج
من خلال mean()
 نحصل على معدل إعادة الطلب لكل منتج. مثلاً، إذا كان معدل reordered 0.6
فهذا يعني أن 60% من المرات التي تم فيها شراء المنتج تم إعادة طلبه

*-average position in cart: هنا حسبنا متوسط موقع المنتج في السلة الشرائية
لما نوخد ال Mean بنعرف المنتج وين بكون ترتيبه بالسله عادة (هل هو من الاساسيات اللي بتنحط اول اشي ولا منن الكماليات بالاخر )


*popularity over time: تم حساب عدد المرات التي تم فيها طلب المنتج باستخدام دالة count على عمود user_id
 هذا يعكس شعبية المنتج بناءً على تكرار عمليات الشراء

-'User×Product interaction features'
الفكرة منه انه نفهم قديش المستخدم متعود يشتري منتج معين وقديش احتمال يعيد شراءه ومتى اخر مره اشتراه 
days_col = 'days_since_prior_order' if 'days_since_prior_order' in df.columns else 'user_days_since_last_order'
هذا السطر معمول احتياط بس، عشان نختار اسم العمود الموجود فعليا للأيام بين الطلبات خاصة انه ممكن يختلف بعد الميرج وهيك 
جمعنا الفيتشر بناءا على اليوزر والبرودكت 
ودمجنا عدد الطلبات الكلي للمستخدم 
uxp_features['uxp_order_rate'] حسبنا هون معدل ظهور المنتج ضمن طلبات المستخدم 
uxp_features['uxp_orders_since_last_bought']:حسبنا عدد الطلبات منذ اخر مرة اشترى المنتج  اذا كان صفر يعني اشتراه ب اخر طلب واذا كان رقم كبير صارله فترة ما اشتراه
-'Temporal Features'
• Temporal features: hour: هنا استخدمنا ال hour_of_day من الداتا سيت الاصلية
 لانه بيعطينا فكرة عن الوقت الي المستخدم بيطلب فيه المنتج
• Temporal features: day: هنا استخدمنا ال day_of_week من الداتا سيت الاصلية 
    لانه بيعطينا فكرة عن اليوم الي المستخدم بيطلب فيه المنتج
• Temporal features: Year: في الداتا سيت الاصلية لم تكن السنة موجودة لذلك افترضنا ان بداية السنه هي 2024 وبرضو استخدمنا عدد ايام السنة 365 
ف يلي عملنا انه حسبنا السنة+اعتمادا على days_since_prior_orderعدد الايام منذ الطلب السابق
وبعدين قسمناها على 365 عشان نحصل على السنة
• Temporal features: Month: بما أن البيانات الأصلية لا تحتوي على معلومات عن الشهر، قمنا بحسابه استنادًا إلى عدد الأيام منذ الطلب السابق (days_since_prior_order)
وقسمناها على 30 للحصول على تقدير تقريبي للشهر بعد ذلك، أضفنا 1 لتحديد الشهر الفعلي في السنة
• Temporal features: Season: قمنا بتحديد الموسم بناءً على الشهر الذي حسبناه سابقًا
• Temporal features: Is_Weekend: قمنا بإنشاء عمود جديد لتحديد ما إذا كان الطلب قد تم في عطلة نهاية الأسبوع ام لا اذا كان 1 او 0 فيكون عطلة نهاية الاسبوع يعني يوم السبت والاحد
• Temporal features: time_of_day : قمنا بتصنيف الوقت من اليوم إلى فئات مختلفة (صباحًا، ظهرًا، مساءً، ليلًا)"اضافيةة"
 بناءً على hour_of_day لتوفير معلومات إضافية حول توقيت الطلبات

-"User * Product -Recent Orders Feature "
ضفنا هاد لجزء لانه لما طبقنا على الموديلز ظهر عنا شوية اندر فيت ف بدنا اشي يزيد الاكيورسي ف ضفنا هاد الفيتشر 
شو عملنا؟
بهاد الجزء حسبنا اخر ميزات تركز على اخر 5 طلبات طبعا استخدمنا rank 
عشان نحدد اخر الطلبات لكل مستخدم بشكل صحيح  وطبعا بكوم مرتب من الاحدث للأقدم  بعدين اخترنا اخر 5 طلبات لكل مستخدم 

-"User * Department Interaction" 
هاد ضفناه برضو عشان نرفع الاكيورسي
ضفنا ميزتين رئيسيتين:
'user_dept_reorder_ratio'-نسبه اعادة شراء المستخدم من هاد القسمم 
'user_dept_total_bought'- عدد المنتجات اللي اشتراها المستخدم من  القسم 
بعدين دمجنا هاي الميزات مع الترين وعالجنا قيم نان ب 0 اذا المستخدم اول مره بشتري من قسم معين 

-"Aggregation features last 3 orders"
في هذه المرحلة قمنا بإنشاء ميزات تجميعية بناءً على آخر 3 طلبات لكل مستخدم
الشرح:تم حساب تجميعات البيانات ب استخدام النوافذ الزمنيه حبث تم التركيز على اخر 3طلبات لكل مستخدم لتحليل سلوكه الاخير
تم تجميع البيانات حسب ال user_id و order_number
لحساب حجم السلة الشرائية (basket_size) وعدد الأيام بين الطلبات (days_gap) 
ثم تم اعادة تسمية الاعمدة لتوضيح المعاني 
بعد لك استخدمنا دالة rolling(window=3, min_periods=1).mean()
لحساب المتوسط المتحرك لآخر 3 طلبات لكل مستخدم
تم تطبيق هذه الدالة على عمود basket_size و days_gap
و استخدام min_periods=1 لضمان ان المستخدمين الجدد الذين لديهم طلب واحد او اثنين فقط لا يتم استبعادهم من التحليل
في النهاية تم .last للحصول على احدث حالة للمستخدم 
هسا لانه train_df يتم بناؤه تدريجيا ومع اعادة تشغيل النوتبوك اكثر من مرة فمن الممكن ان تكون هذه الاعمدة موجودة مسبقا داخل الترين دف ولكن بقيمة قديمه 
لذلك قمنا بحذفها في حاللل كانت موجودة وعشان يعني نتأكد انه ما منستخدم نسخة خاطئه ثم 
تم اضافتها بعد حسابها من البرايور فقط وهييك بتكون لحسابات صحيحة

-"At least one engineered non-linear feature (log transforms, polynomial, interaction terms)."
في هذه المرحلة قمنا بإنشاء ميزات غير خطية باستخدام التحويل اللوغاريتمي  
طبقت هذا التحويل على days_since_prior_order 
بهدف تقليل التوزيع المنحرف وتحويله الى توزيع الطبيعي 
استخدمناه لانه في انحرافات  يعني في ناس كثير بتغيب يوم، وناس قليل بتغيب 30 يوم
حيث أن القيم الكبيرة في هذا العمود يمكن أن تؤدي إلى تأثيرات غير مرغوب فيها عند تدريب النماذج
ستخدمنا دالة log1p 
التي تحسب log(x+1)
 حيث أن هذا يساعد في تجنب القيم السالبة أو القيم صفرية
 #OWEN
دخلت على عملية البري بروسيسنق بعد ما خلصت الانجنيرينق وخليت الفيتشرز المطلوبة مني بس وقسمت الداتا بناءا على الطلب الاخير
بحيث انه اذا هاذ الطلب الاخير  = test
اذا مش الطلب الاخير =train
وحذفت الكولومز الي هي ممكن تعمل نويز وتخرب على اداء الموديل مثل ال ids و اكم فيتشر موجودين بالكود 
بعدها عملت البري بروسيسور داخل بايبلاين بحيث انه امتنع عن اي  data leakage
وبرضو كذالك اي موديل كنت اعمله داخل بايب لاين بحيث ما يصير اي leakage
-------------------------------------------------------------------------------------------------------------------------------------------
     
                                                 MODELS CLASSIFY


1-knn -> بالبداية الكود دخلت على المودل كثيررر فيتشرز مما ادى الى انه يصير عندي اوفر فيت ف اضطريت انه ازيد قائمة الحذف وال 
ال knn بطبعه حساس فيعني النتيجة العالية الي بينت معنا ناتجة عشان طريقة عمله هو بس بعتمد عالميموري والمسافات هو ما بعتمد اي معادلة رياضية معينة يعني عشان هيك طلع معنا 0.89
وبعد ما عملت فيتشر امبورتنس وشلت كم فيتشر اتحسن الموديل وراح الليكج وصارت الاكيورسي 0.78

2- Logistic regrresion
هون الوضع كان افضل و generlized اكثرر ومع تقليل الفيتشرز اتحسن اكثر واكثر وجاب تقريبا 0.81
وهاذ دليل انه ما في اي leakage عندي لانه كل المودلز مدربة على نفس البري بروسيس فيعني شغلنا تمام

3- Random forest (Score) لانه والله ضل يطلعلي memory error فما قدرت الا اجيبله السكور
هو بطبعه قوي وسريع لانه بشتغل على كثيرررر trees  وبختار احسن ومع احسن باراميترز جربته فيه جاب اكيورسي تقريبا 84.3

4- linear svm
بالبداية كان ضعيففففففف كثير بس دربته على نفس الفيتشرز الي دربت عليهم الي قبل فاضطريت اعملله قائمة خاصة واقلل منهممم بحيث انه يصير يعمم احسن يعني فعمل ال svm_col وبس عملتله ودربته وجربته طلع معي تقريبا 
0.79

5- dicision tree
هي بطبعها عالية فاضطريت. انه احصر ال depth تبعها واوصل لاحسن نتيجة بالباراميترز 
0.74 واصريت انه ما يصير عندي ليك ايج فوصلت 

6- xgboost 
هاذ اقوى واسرع موديل ومعروف ما عندي خبرة كثير فيه صراحة لانه ما اخذناه بالمادة بس عالعموم عنفس البروسيس وعنفس الشغل طلع
0.78 مش عارف ليه صراحة مع انه لازم اقوى بس ممقارنة بالداتا يعتبر كويس جدا
- lightGBM
هاذ سريع برضو وطلعت معنا الاكيورسي اله وقوي صراحة طلع وبدون ليكج ومع اكيورسي ممتازة
0.82
# وعملت Thershold للمودلز الي حسيت صار فيها تحيز لمودل معين واتحسن عندي التعميم كثير 
+ عملت امبورتنس فيتشر للمودلز الي حسيتها استصعصت وشلت منها كم فيتشر 


# طبعا عملنا لكل المودلز ال بلوتس اللاومة والمقارانات 
(التجارب موجودة بالكود )
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
*                                                             MODELS REGREESION



# KNN
اموره ممتازة كانت وعملت اله لوق عشان اقلل حجم البيانات عشان يقدر يتنبا احسن وافضل 
MAE: 2.356940507888794
RMSE: 15.84200668334961
R2: 0.8352253842353821

----------------------------------------------------------------------------------------------------------------
# كل الي تحت ما ضل الوقت الكافي اني اعمللهم تونينق بس مع هيك نتائجهم مش سيئة

# Decision Tree
اعملته على البريبروسيسور الاصلي ونتيجته 
Decision Tree MAE: 3.2700218874141735
Decision Tree RMSE: 25.92966236104664
Decision Tree R2: 0.7630377500459213
-----------------------------------------------------------------------------------------------------------------

# Random Forest 
عملتله بريبروسيسور خاص وكولمز خاصين لحتى اطلع بنتيجة مقبولة شوي وعشان ضيق الوقت

RF MAE: 3.399033443682533
RF RMSE: 27.214095739185904
RF R2: 0.7512997559694063
-----------------------------------------------------------------------------------------------------------------
# XGBOOST
عملتله بريبروسيسور خاص وكولمز خاصين لحتى اطلع بنتيجة مقبولة شوي وعشان ضيق الوقت
XGB MAE: 3.4658944606781006
XGB RMSE: 24.896343231201172
XGB R2: 0.7724809050559998
-----------------------------------------------------------------------------------------------------------------
Light GBM
عملتله بريبروسيسور خاص وكولمز خاصين لحتى اطلع بنتيجة مقبولة شوي وعشان ضيق الوقت
LightGBM MAE: 3.408399696570057
LightGBM RMSE: 24.835648339438738
LightGBM R2: 0.7730355672342765
--------------------------------------------------------------------------------------------------------------------
عملنا كل البلوتس اللازمة للمودلز +shap + مقارنات وملفات بي دي اف مع الادلة

